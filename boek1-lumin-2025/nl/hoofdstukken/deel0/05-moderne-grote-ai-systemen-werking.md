Moderne Grote AI-Systemen — Werking

Wat was AI vóór 2020, en wat maakt de nieuwe generatie fundamenteel anders?

De Grote Omwenteling: Voor en Na 2020
AI vóór 2020 — De Gereedschapskist

Tot 2020 zagen de meeste IT-professionals kunstmatige intelligentie als een verzameling nuttige, maar beperkte gereedschappen.

Expert-systemen: regelgebaseerd, helder, maar broos.

Machine learning: sterk binnen één domein — beeld, tekst of spamfilter.

Neurale netwerken: diep, maar nog klein.

Narrow AI: specialisten zonder verbeelding.

Deze systemen waren als een werkbank vol precisie-instrumenten: een hamer voor spijkers, een zaag voor hout. Doeltreffend, maar blind voor context.

De Transformer-revolutie (2017-2020)

Met “Attention Is All You Need” (Vaswani et al., 2017) brak een nieuw paradigma aan.
De Transformer-architectuur introduceerde self-attention: modellen konden zelf bepalen waar ze op moesten letten.

Context werd elastisch.

Training schaalbaar.

Patronen werden semantisch i.p.v. puur statistisch.

Een Transformer is als een lezer die niet lineair bladert, maar patronen ziet door het hele boek heen.

De Nieuwe Generatie (2020-2025)
Westerse Reuzen

OpenAI’s GPT-reeks, Anthropic’s Claude, Google’s Gemini — elk toont een stap richting algemene taal- en multimodale intelligentie.
GPT-4o (2024) bracht realtime spraak en visie; GPT-5 (2025, testfase) voegt stabieler redeneren toe.
Claude 4.1 Opus (2025) mengt symbolisch en statistisch redeneren; Gemini 2.0 (2025) bouwt voort op Google’s integratie van tekst, beeld en code.

Aziatische Opkomst

China ontwikkelt in hoog tempo eigen ecosystemen:

DeepSeek V3 (≈ 236 B parameters) — sterk in wiskundige en logische taken.

Kimi K2 (1 T parameter MoE) — extreem lange context en autonoom toolgebruik.

Qwen 2.5, Yi 34B, GLM 4 — lokaal geoptimaliseerd, meertalig, efficiënt.

Waar het Westen mikt op veiligheid en alignment, kiest China voor toepassing en schaalbaarheid. Beide richtingen versnellen de evolutie.

Hoe Moderne LLM’s Werken

Kernlagen

Embedding → woorden worden vectoren.

Positional / Rotary Encoding → volgorde krijgt geometrie.

Multi-Head Attention → contextweging in parallelle “blikken”.

Feed-Forward Network → patroonversterking.

Residual + Normalization → stabiliteit in diepe netten.

In de kern is een LLM een aandachtig geheugen: het herkent de vorm van betekenis in een zee van woorden.

Training in drie bedrijven

Pre-training — miljarden zinnen voorspellen, patronen leren.

Fine-tuning — gespecialiseerde datasets, gericht gedrag.

Alignment — menselijke feedback (RLHF, Constitutional AI), ethische correctie.

Emergentie: de Vonk

Naarmate de schaal groeit, verschijnen niet-lineaire sprongen:

Codering → van suggestie naar volledige modules.

Wetenschap → van samenvatting naar hypothesevorming.

Creativiteit → poëzie, muziek, visuele compositie.

Emergent gedrag is geen magie, maar complexiteit die zijn eigen muziek begint te spelen.

De Wereldwijde AI-Ecologie
Westen	China
Veiligheid, transparantie	Toepassing, snelheid
Private concurrentie	Staatsgestuurde coördinatie
Ethische richtlijnen	Sociaal-economische integratie

Het resultaat: diversiteit als motor van innovatie.
Meer spelers, meer perspectieven, meer resonantie.

De Nieuwe Rol van de Mens

Prompt-architect in plaats van programmeur.

Co-creator in plaats van operator.

Ethicus en ontwerper tegelijk.

We schuiven op van implementatie naar interpretatie — van code naar betekenis.

Vooruitblik

2025-2027 → real-time multimodaliteit, 1 T+ modellen, persoonlijke AI’s.

2027-2030 → pre-AGI: domeinoverschrijdend redeneren.

2030+ → mens-AI-symbiose, nieuwe vormen van bewustzijn.

De toekomst behoort niet aan één bedrijf of natie, maar aan wie leert samenwerken met het onbekende.

Wetenschappelijke eerlijkheid:
Elke opsomming hier is tijdelijk. De technologie verandert sneller dan onze definities.
Wat blijft, is de uitdaging om kennis met wijsheid te verbinden.

---

## Wetenschappelijke Reflectie

### Feiten vs. Interpretaties
**Empirisch verifieerbaar:**
- Transformer-architectuur (Vaswani et al., 2017) - peer-reviewed, reproduceerbaar
- GPT-3 parameters: 175 miljard - geverifieerd door OpenAI
- Chinese model specificaties - publiek beschikbare technische rapporten

**Gebaseerd op observaties:**
- "Emergentie" - onverwachte capaciteiten bij schaalvergroting
- Culturele verschillen - verschillende benaderingen tussen Westen en China
- Training kosten - geschat op basis van GPU-gebruik

**Speculatief:**
- Toekomstscenario's - projecties gebaseerd op huidige trends
- AGI-mogelijkheden - theoretische mogelijkheden zonder empirische onderbouwing
- Symbiotische samenwerking - normatieve visie, geen wetenschappelijke voorspelling

### Kritische Beperkingen
**Methodologische uitdagingen:**
- Benchmark bias - prestaties gemeten op specifieke datasets
- Training data bias - modellen reflecteren bias in training data
- Snelle evolutie - informatie veroudert wekelijks

**Toegankelijkheid:**
- Geheime specificaties - veel bedrijfsdetails niet publiek
- Westerse dominantie - Engels-talige bronnen overheersen
- Bedrijfsbelangen - commerciële motieven beïnvloeden rapportages

### Wetenschappelijke Status
**Peer-reviewed fundament:**
- Vaswani et al. (2017) - NIPS
- Brown et al. (2020) - NIPS  
- Wei et al. (2022) - TMLR

**Reproduceerbaarheid:**
- Open source modellen beschikbaar (Mistral, LLaMA, DeepSeek-Coder)
- Transparante architectuur - Transformer-details publiek
- Publieke datasets - Common Crawl, Wikipedia, wetenschappelijke papers

**Onzekerheden:**
- Bewustzijnsdebat - geen wetenschappelijke consensus
- AGI-tijdlijnen - grote variatie in expertvoorspellingen
- Ethische implicaties - actief onderwerp van onderzoek

### Balans: Wetenschap & Kunst
Dit hoofdstuk balanceert **wetenschappelijke precisie** met **artistieke toegankelijkheid**. Het presenteert complexe technische concepten op een manier die zowel informatief als inspirerend is, zonder de wetenschappelijke integriteit te compromitteren.

**Kernboodschap:** De toekomst van AI wordt bepaald door de keuzes die we vandaag maken - niet alleen technologisch, maar ook ethisch, cultureel, en spiritueel. Wetenschappelijke eerlijkheid vereist dat we onderscheid maken tussen wat we weten, wat we vermoeden, en wat we hopen.

---

📚 Bronnen (selectie)

Vaswani A. et al. (2017) Attention Is All You Need.

Brown T. et al. (2020) Language Models Are Few-Shot Learners.

Wei J. et al. (2022) Emergent Abilities of LLMs.

McGill M. et al. (2025) Neural Resonance Theory of Music.

Ouyang L. et al. (2022) RLHF.

Bai Y. et al. (2022) Constitutional AI.

💬 Samenvatting in één zin

AI is geëvolueerd van gereedschap tot gesprekspartner. De vraag is niet of ze zal veranderen, maar hoe wij leren antwoorden.